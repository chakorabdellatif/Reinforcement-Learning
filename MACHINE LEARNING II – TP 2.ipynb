{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c71e64-aba1-456e-9dbb-0a2352dbca4c",
   "metadata": {},
   "source": [
    "# Exploration de l'environnement FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4b28de4-23c7-4bb3-b273-9dd573e5194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c13984ea-385c-4524-a9ca-9ea8f95bfad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# charger l'environnement FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery = True)# render_mode=\"human\"\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb628ef1-df84-4ec5-99af-59db93ae77d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d’actions : Discrete(4)\n",
      "Espace d’observations : Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# afficher les info de env\n",
    "print(f\"Espace d’actions : {env.action_space}\")  \n",
    "print(f\"Espace d’observations : {env.observation_space}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27645ba-be20-495a-8a55-4bde80bf9925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 3, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n",
      "Action : 3, Observation : 1, Reward : 0.0\n",
      "Action : 0, Observation : 0, Reward : 0.0\n",
      "Action : 1, Observation : 1, Reward : 0.0\n"
     ]
    }
   ],
   "source": [
    "# executer une boucle des actions aleatoires\n",
    "for _ in range(10):  \n",
    "    action = env.action_space.sample()  \n",
    "    observation, reward, done, _, _ = env.step(action)  \n",
    "    print(f\"Action : {action}, Observation : {observation}, Reward : {reward}\")  \n",
    "    if done:  \n",
    "        env.reset()  \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846aea1-fa4d-424c-b8d1-7dfe73efa386",
   "metadata": {},
   "source": [
    "# Exercice 2 : implementation de Q-table et Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df51fd5-d441-4b4e-aac3-f4e201473fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table initialisee :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initaialisation de la Q-table\n",
    "import numpy as np\n",
    "q_table = np.zeros(shape = (16,4))\n",
    "# affichage de la Q_table initaile\n",
    "print(\"Q-Table initialisee :\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10560713-69a2-4d76-b9c3-69b11afb7fa0",
   "metadata": {},
   "source": [
    "# Implementation du Q-Learning avec Mise a Jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e20f861a-d858-48f8-96d0-3787afe6c6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table après apprentissage :\n",
      "[[0.55639479 0.47699659 0.48001039 0.46972858]\n",
      " [0.27388889 0.23411987 0.32660941 0.49635747]\n",
      " [0.36861881 0.36725727 0.37624745 0.44421719]\n",
      " [0.26245155 0.19624051 0.18818505 0.41961375]\n",
      " [0.57325178 0.37912639 0.29525849 0.43044168]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32692174 0.16940392 0.16355878 0.05195193]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33959929 0.35143203 0.34662802 0.59986506]\n",
      " [0.46899535 0.64699365 0.53448723 0.42500232]\n",
      " [0.62404496 0.28715617 0.3934711  0.37347633]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.43539897 0.45047413 0.76210369 0.49129449]\n",
      " [0.7019102  0.89950805 0.73698217 0.67664597]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Parametres\n",
    "alpha = 0.1 # Taux d'apprentisage\n",
    "gamma = 0.99 # Facteur de discount\n",
    "epsilon = 1.0 # Exploration initiale\n",
    "epsilon_decay = 0.995 # Decroissance d'epsilon\n",
    "num_episode = 1000 # Nombre d'episode\n",
    "\n",
    "\n",
    "# Boucle d'apprentissage\n",
    "for episode in range(num_episode):\n",
    "    # Réinitialisation de l'environnement à chaque épisode\n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Exploration (action aléatoire)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploitation (meilleure action selon la Q-table)\n",
    "\n",
    "        # Prendre l'action et observer la récompense et le nouvel état\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Mise à jour de la Q-table en utilisant la règle de Q-learning\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
    "\n",
    "        # Passer à l'état suivant\n",
    "        state = next_state\n",
    "\n",
    "    # Décroître epsilon pour réduire progressivement l'exploration\n",
    "    epsilon = max(epsilon * epsilon_decay, 0.01)  # On garde epsilon au moins à 0.01\n",
    "\n",
    "# Afficher la Q-table finale après l'apprentissage\n",
    "print(\"Q-Table après apprentissage :\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828df03-b626-46ae-81f1-c88f4d875652",
   "metadata": {},
   "source": [
    "# Evaluation des Performences de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "392f83a5-8240-4918-a8f9-06cefd7ea200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de réussite sur 100 épisodes : 85.00%\n"
     ]
    }
   ],
   "source": [
    "# Lancer 100 épisodes en utilisant l'action optimale et mesurer le taux de réussite\n",
    "success_count = 0  # Compteur de réussites\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env.reset()[0]  # Initialisation de l'état\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Choisir l'action optimale\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        if done and reward == 1:  # Si l'agent atteint l'objectif (reward == 1)\n",
    "            success_count += 1\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "# Calculer et afficher le taux de réussite\n",
    "success_rate = success_count / 100\n",
    "print(f\"Taux de réussite sur 100 épisodes : {success_rate * 100:.2f}%\")\n",
    "\n",
    "# Fermer l'environnement\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
